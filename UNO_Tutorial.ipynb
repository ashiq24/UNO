{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "UNO_Tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "5sLspEnB73-I",
        "YmX7VzmwsNl4"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashiq24/UNO/blob/main/UNO_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# U-shaped Neural operator\n",
        "This tutorial gives a step by step break down on how to use the U shaped Neural Operators (UNO), which is introduced in the paper with the same title U-NO: [U-shaped Neural Operators](https://arxiv.org/pdf/2204.11127.pdf). This \n",
        "\n",
        "First we need to download few files from the github repository of UNO. The file **integral_operators.py** contrains non-linear integral operators, the building block of U-NO."
      ],
      "metadata": {
        "id": "6QbHWRs1Pa90"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPjNx0AvASYk"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/ashiq24/UNO/main/Adam.py\n",
        "!wget https://raw.githubusercontent.com/ashiq24/UNO/main/utilities3.py\n",
        "!wget https://raw.githubusercontent.com/ashiq24/UNO/main/integral_operators.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.parameter import Parameter\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import operator\n",
        "from functools import reduce\n",
        "from functools import partial\n",
        "from timeit import default_timer\n",
        "from utilities3 import *\n",
        "from Adam import Adam\n",
        "from torchsummary import summary\n",
        "import gc\n",
        "import math\n",
        "from integral_operators import * "
      ],
      "metadata": {
        "id": "JvFZ5sJEBVef"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example\n",
        "Let's we have a function $f(x,y) = [x^2+y, x+y^2]$ where $(x,y) \\in (0,1)^2$. So, it's domain is $2D$ and co-domain dimension is also 2. \n",
        "\n",
        "We will apply non-linear operator $G$ (linear integral operators\n",
        "followed by point-wise non-linearity) on the function $f$ such that $g = G(f)$.\n",
        "\n",
        "Where, $g$ is defined on domain $(0.0,0.5)^2$ with 4 dimentional co-domain i.e.,\n",
        "\n",
        " $g: (0.0.5)^2 \\to R^4$\n",
        "\n",
        "For this example, we will use a non-linear operator perfroming $2D$ integral operation. It is implemented as the class **OperatorBlock_2D** in **integral_operators.py** module. It uses **SpectralConv2d_Uno** for kernel integration and **pointwise_op_2D** for point-wise operator. Details can be found in the paper.\n",
        "\n",
        "##Discretization\n",
        "To work with the input function $f$ anlytically, we need to discretize its' domian $(0,1)^2$. We will discretize the domain with a grid fo size $100 \\times 100$ (sampling rate 0.01).\n",
        "\n",
        "Following the same sampling rate, the output function with domain $(0.0,0.5)^2$ will have a grid size of $50 \\times 50$\n",
        "\n",
        "##Domain Contraction and Expansion\n",
        "\n",
        "Note that by using operator $G$ we are contracting the domian of input function $f$ by a factor of $0.5$  ($(0,1)^2$ to $(0.0,0.5)^2$). This contraction factor can also be expressed a the ratio of grid size along each dimension of the domain the input and functions (100 to 50).\n",
        "\n",
        "\n",
        "\n",
        "Now let's get to the code."
      ],
      "metadata": {
        "id": "YZgBXqiNXxUm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create the input function"
      ],
      "metadata": {
        "id": "z0AtVEeFfGqh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def func(x, y):\n",
        "    return [x**2+y,x+y**2]\n",
        "\n",
        "xaxis = np.linspace(0, 1, 100)\n",
        "yaxis = np.linspace(0, 1, 100)\n",
        "function_f = func(xaxis[:,None], yaxis[None,:])"
      ],
      "metadata": {
        "id": "Zl6Ej18ACuV9"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f  = torch.tensor(function_f)"
      ],
      "metadata": {
        "id": "SB5wupgkgTuJ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f.shape # note that co-domain dimension is at the begining."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5WMsYeu5gVRX",
        "outputId": "710b4974-6307-4d5a-89ef-c09c503f9b6c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 100, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will initialize our operator $G$. We need to pass several parameters to the class OperatorBlock_2d.\n",
        "\n",
        "###**OperatorBlock_2d(in_codim, out_codim,dim1, dim2,modes1,modes2, Normalize = False,Non_Lin = True)**\n",
        "        \n",
        "1.   in_codim = co-domain dimension of input function (for $f$ this is 2)\n",
        "2.   out_codim = co-domain dimension of output function (for $g$ this is 4)\n",
        "3.   dim1 = Default output grid size along $x$ axis of the output function $g$. Which is 50 in out case.\n",
        "4.   dim2 = Default output grid size along $y$  axis of the output function $g$. This is also 50.\n",
        "5. modes1 and modes2 = Number of fourier model the operator will use to perform the kernel integration (we set both of them as 10)\n",
        "6. Normalize = If set to True performs InstanceNorm2d, by default it is set to False\n",
        "7. Non_Lin = If True, applies point wise nonlinearity (Gelu). We can set it to False if we need liniear operator.\n",
        "\n",
        "\n",
        "#### Note:\n",
        "Neural operator is discretization invarient. That means, the same operator can handle the function $f$ with discretization of grid size (100x100) and also with iscretization of grid size (1000x1000). The discretization of output function $g$ will also be changed in the same way.\n",
        "\n",
        "But often time we train the model with fixed discretization. The option for default output grid size is kept considering the ease in coding duirng traing the neural operators.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4oR7X_ORi-H-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# intizalizing the oprator\n",
        "G = OperatorBlock_2D(2,4,50,50,10,10)\n",
        "f = f.reshape(1,2,100,100) #adding a batch dimension\n",
        "g = G(f.float())\n",
        "print(g.shape)"
      ],
      "metadata": {
        "id": "ZPOeAFHggc3Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48bda24f-49fe-4f9c-cb6a-a6091286dc3f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 4, 50, 50])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The forward function of the class **OperatorBlock_2D** also has two other parameter fixing the grid size of the output function.\n",
        "\n",
        "In other words, we can also calculate $g$ in the following way."
      ],
      "metadata": {
        "id": "lG9AKV_vq8WI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "g = G(f.float(), dim1 = 50, dim2 = 50)\n",
        "print(g.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGz3mk-1qwTd",
        "outputId": "07939687-2c2b-4275-ac06-21bba4d64924"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 4, 50, 50])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Breaking down the Operator Block.\n",
        "\n",
        "**OperatorBlock_2D** uses **SpectralConv2d_Uno** and **pointwise_op_2D** for performing the non-linear integral operation.\n",
        "\n",
        "The operator action in the pervious cell can be broken down as the following\n",
        "\n"
      ],
      "metadata": {
        "id": "5sLspEnB73-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "I = SpectralConv2d_Uno(2,4,50,50,10,10) # Initializing the kernel integral operator\n",
        "p = pointwise_op_2D(2,4,50,50) #Initializing the pointwise operator\n",
        "\n",
        "g = F.gelu(I(f.float())+ p(f.float()))\n",
        "print(g.shape)"
      ],
      "metadata": {
        "id": "mqxk0azOoo8y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc64302e-5d37-40fb-8b29-45d265d86d10"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 4, 50, 50])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Handling finer discretization\n",
        "Now, assume that we recive the input function with finer discreetization over the input domain (grid size 1000x 1000).\n",
        "\n",
        "Maintaining the contraction ratio, the grid size of the output function $g$ will be $500 \\times 500$. Now, we can use the same operator $G$ to get the output function $g$ with finer discretization."
      ],
      "metadata": {
        "id": "YmX7VzmwsNl4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#input function with finer discretization.\n",
        "xaxis = np.linspace(0, 1, 1000)\n",
        "yaxis = np.linspace(0, 1, 1000)\n",
        "function_f_finer = func(xaxis[:,None], yaxis[None,:])\n",
        "f_finer  = torch.tensor(function_f_finer)\n",
        "print(f_finer.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6SOrX3hxsNDf",
        "outputId": "fc374d11-70f5-45ba-d037-eb8e555f3479"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 1000, 1000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " We can use the operator **G**, previously intitialized, to get the output function $g$. \n",
        "\n",
        "Note, as the discretiztion of **g** is not the same as the default grid size defined during the initializtion of **G**, we need specify it in the function call ***G()***."
      ],
      "metadata": {
        "id": "_bw4wGsfvzms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f_finer = f_finer.reshape(1,2,1000,1000)\n",
        "g_finer = G(f_finer.float(), dim1 = 500, dim2 = 500)\n",
        "print(g_finer.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUQum7d0vwJ0",
        "outputId": "06ea7b0c-e6de-49fb-ec82-c82e29b46810"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 4, 500, 500])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating and Training a U-NO\n",
        "\n",
        "There are three main components in U-NO\n",
        "\n",
        "1. Lifting operator P (Lift the input function to higher dimentional Co-domain)\n",
        "2. Stacked non-linear integral operator G\n",
        "3. Projection operator Q (Project the function to lower dimensional Co-domain) \n",
        "\n",
        "####Things to note:\n",
        "1. The co-domain dimension should be kept at the end. So, the function $f$ used above as an example will have the shape (100,100,2) after discretization with grid size 100x100. \n",
        "2. Following the paper we will also concatenate position (x & Y) along the co-domain dimesion.\n",
        "3. If the function is non-periodic, we extend the domain by zero padding.\n",
        "\n",
        "In the following we will create a UNO 2D model."
      ],
      "metadata": {
        "id": "v1VB9jbPKqKE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UNO_demo(nn.Module):\n",
        "    def __init__(self, in_width, width,pad = 8):\n",
        "        super(UNO_demo, self).__init__()\n",
        "   \n",
        "        self.in_width = in_width # input function co-domain dimention after concatenating (x,y)\n",
        "        self.width = width # lifting dimension\n",
        "        \n",
        "        self.padding = pad  # passing amount\n",
        "\n",
        "        self.fc = nn.Linear(self.in_width, self.width//2)\n",
        "\n",
        "        self.fc0 = nn.Linear(self.width//2, self.width) \n",
        "\n",
        "        self.G0 = OperatorBlock_2D(self.width, 2*self.width,40, 40, 20, 20)\n",
        "\n",
        "        self.G1 = OperatorBlock_2D(2*self.width, 4*self.width, 20, 20, 10,10)\n",
        "\n",
        "        self.G2 = OperatorBlock_2D(4*self.width, 8*self.width, 10, 10,5,5)\n",
        "        \n",
        "        self.G3 = OperatorBlock_2D(8*self.width, 16*self.width, 5, 5,3,3)\n",
        "        \n",
        "        self.G4 = OperatorBlock_2D(16*self.width, 16*self.width, 5, 5,3,3)\n",
        "        \n",
        "        self.G5 = OperatorBlock_2D(16*self.width, 16*self.width, 5, 5,3,3)\n",
        "        \n",
        "        self.G6 = OperatorBlock_2D(16*self.width, 16*self.width, 5, 5,3,3)\n",
        "        \n",
        "        self.G7 = OperatorBlock_2D(16*self.width, 16*self.width, 5, 5,3,3)\n",
        "        \n",
        "        self.G8 = OperatorBlock_2D(16*self.width, 16*self.width, 5, 5,3,3)\n",
        "        \n",
        "        self.G9 = OperatorBlock_2D(16*self.width, 8*self.width, 10, 10,3,3)\n",
        "        \n",
        "        self.G10 = OperatorBlock_2D(16*self.width, 4*self.width, 20, 20,5,5)\n",
        "\n",
        "        self.G11 = OperatorBlock_2D(8*self.width, 2*self.width, 40, 40,10,10)\n",
        "\n",
        "        self.G12 = OperatorBlock_2D(4*self.width, self.width, 80, 80,20,20) # will be reshaped\n",
        "\n",
        "        self.fc1 = nn.Linear(1*self.width, 2*self.width)\n",
        "        self.fc2 = nn.Linear(2*self.width, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        grid = self.get_grid(x.shape, x.device)\n",
        "        x = torch.cat((x, grid), dim=-1) # concatenating position (x,y) along the co-domain\n",
        "\n",
        "        x_fc = self.fc(x)\n",
        "        x_fc = F.gelu(x_fc)\n",
        "\n",
        "        x_fc0 = self.fc0(x_fc)\n",
        "        x_fc0 = F.gelu(x_fc0)\n",
        "        \n",
        "        x_fc0 = x_fc0.permute(0, 3, 1, 2)\n",
        "        x_fc0 = F.pad(x_fc0, [0,self.padding, 0,self.padding])\n",
        "        \n",
        "        D1,D2 = x_fc0.shape[-2],x_fc0.shape[-1]\n",
        "\n",
        "        x_c0 = self.G0(x_fc0,D1//2,D2//2)\n",
        "\n",
        "        x_c1 = self.G1(x_c0,D1//4,D2//4)\n",
        "\n",
        "        x_c2 = self.G2(x_c1,D1//8,D2//8)\n",
        "  \n",
        "        x_c3 = self.G3(x_c2,D1//16,D2//16)\n",
        "        \n",
        "        x_c4 = self.G4(x_c3,D1//16,D2//16)\n",
        " \n",
        "        x_c5 = self.G5(x_c4,D1//16,D2//16)\n",
        "\n",
        "        x_c6 = self.G6(x_c5,D1//16,D2//16)\n",
        "        \n",
        "        x_c7 = self.G7(x_c6,D1//16,D2//16)\n",
        "        \n",
        "        x_c8 = self.G8(x_c7,D1//16,D2//16)\n",
        " \n",
        "        x_c9 = self.G9(x_c8,D1//8,D2//8)\n",
        "        x_c9 = torch.cat([x_c9, x_c2], dim=1) \n",
        "        \n",
        "        x_c10 = self.G10(x_c9 ,D1//4,D2//4)\n",
        "        x_c10 = torch.cat([x_c10, x_c1], dim=1)\n",
        "\n",
        "        x_c11 = self.G11(x_c10 ,D1//2,D2//2)\n",
        "        x_c11 = torch.cat([x_c11, x_c0], dim=1)\n",
        "\n",
        "        x_c12 = self.G12(x_c11,D1,D2)\n",
        "        if self.padding!=0:\n",
        "            x_c12 = x_c12[..., :-self.padding, :-self.padding]\n",
        "\n",
        "\n",
        "        x_c12 = x_c12.permute(0, 2, 3, 1)\n",
        "\n",
        "        x_fc1 = self.fc1(x_c12)\n",
        "        x_fc1 = F.gelu(x_fc1)\n",
        "        x_out = self.fc2(x_fc1)\n",
        "        \n",
        "        return x_out\n",
        "    \n",
        "    def get_grid(self, shape, device):\n",
        "        batchsize, size_x, size_y = shape[0], shape[1], shape[2]\n",
        "        gridx = torch.tensor(np.linspace(0, 1, size_x), dtype=torch.float)\n",
        "        gridx = gridx.reshape(1, size_x, 1, 1).repeat([batchsize, 1, size_y, 1])\n",
        "        gridy = torch.tensor(np.linspace(0, 1, size_y), dtype=torch.float)\n",
        "        gridy = gridy.reshape(1, 1, size_y, 1).repeat([batchsize, size_x, 1, 1])\n",
        "        return torch.cat((gridx, gridy), dim=-1).to(device)"
      ],
      "metadata": {
        "id": "CejApP4CKvca"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will use some randomly generated data to train the model as an example. We will consider a grid size of (80x80) and co-domain dimesion of 1.\n",
        "\n",
        "For simplicity, we will assume that the output function also has the same domain and discretization."
      ],
      "metadata": {
        "id": "yv0dovy13-dM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a,u = torch.rand((1000,80,80,1)), torch.rand((1000,80,80,1)) # creating 1000 random samples. Note that the co-domian dimension is at the end.\n",
        "train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(a, u), batch_size=20, shuffle=True)"
      ],
      "metadata": {
        "id": "gIR_war6hEHb"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = UNO_demo(3,32)"
      ],
      "metadata": {
        "id": "5WNHQ6lm39wD"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = Adam(model.parameters(), lr=0.0001, weight_decay=0.001,amsgrad = False)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.8)\n",
        "myloss = LpLoss(size_average=False) #it calculates the error rate\n",
        "model.train()\n",
        "for ep in range(1000): #training from 1000 epocs\n",
        "    train_l2 = 0\n",
        "    for x, y in train_loader:\n",
        "        batch_size = x.shape[0]\n",
        "        optimizer.zero_grad()\n",
        "        out = model(x).reshape(batch_size, 80, 80)\n",
        "\n",
        "        loss = myloss(out.view(batch_size,-1), y.view(batch_size,-1))\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        train_l2 += loss.item()\n",
        "        del x,y,out,loss\n",
        "        gc.collect()\n",
        "    scheduler.step()\n",
        "    train_l2/= 1000\n",
        "    print(\"Episode \",ep,\" Traing Error \", train_l2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OiS6ssJEyeiy",
        "outputId": "5dbd47f5-9ad7-4538-f953-673afd1fca10"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode  0  Traing Error  0.9209523067474366\n",
            "Episode  1  Traing Error  0.5054093112945557\n",
            "Episode  2  Traing Error  0.5001014261245728\n",
            "Episode  3  Traing Error  0.5000590000152588\n",
            "Episode  4  Traing Error  0.5000474052429199\n",
            "Episode  5  Traing Error  0.5000405473709106\n",
            "Episode  6  Traing Error  0.5000395154953003\n",
            "Episode  7  Traing Error  0.5000354480743409\n",
            "Episode  8  Traing Error  0.5000341625213623\n",
            "Episode  9  Traing Error  0.5000327596664429\n",
            "Episode  10  Traing Error  0.5000302925109863\n",
            "Episode  11  Traing Error  0.5000303478240967\n",
            "Episode  12  Traing Error  0.5000306797027588\n",
            "Episode  13  Traing Error  0.5000345869064331\n",
            "Episode  14  Traing Error  0.5000315465927124\n",
            "Episode  15  Traing Error  0.5000373487472534\n",
            "Episode  16  Traing Error  0.5000310707092285\n",
            "Episode  17  Traing Error  0.5000308265686035\n",
            "Episode  18  Traing Error  0.5000270404815674\n",
            "Episode  19  Traing Error  0.5000283155441284\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Bm7lAb8i6x8A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}